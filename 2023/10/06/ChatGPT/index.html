<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1"><meta name="keywords" content="Hexo Theme Redefine"><meta name="author" content="Hong Nie"><link rel="preconnect" href="https://fonts.googleapis.com"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link rel="preconnect" href="https://registry.npmmirror.com" crossorigin><link rel="canonical" href="https://gme-hong.github.io/2023/10/06/chatgpt/"><meta name="robots" content="index,follow"><meta name="googlebot" content="index,follow"><meta name="revisit-after" content="1 days"><meta name="description" content="前言作为一名非专精于NLP算法的同学，去详细论述ChatGPT的底层原理是很困难的。但好在最近为了搭建一个轻量化的RGM(Robotic Grasp Model)，接触到了一部分的图像注意力机制。想着CV、NLP本是一家亲，更何况ChatGPT的前世还可以追溯到《Attention is all you need》，为此斗胆写了一篇《ChatGPT 打开潘多拉魔盒，行业壁垒逐步瓦解》的爽文（对！爽"><meta property="og:type" content="article"><meta property="og:title" content="ChatGPT 打开潘多拉魔盒，行业壁垒逐步瓦解"><meta property="og:url" content="https://gme-hong.github.io/2023/10/06/ChatGPT/index.html"><meta property="og:site_name" content="OpticHong&#39;s Blog"><meta property="og:description" content="前言作为一名非专精于NLP算法的同学，去详细论述ChatGPT的底层原理是很困难的。但好在最近为了搭建一个轻量化的RGM(Robotic Grasp Model)，接触到了一部分的图像注意力机制。想着CV、NLP本是一家亲，更何况ChatGPT的前世还可以追溯到《Attention is all you need》，为此斗胆写了一篇《ChatGPT 打开潘多拉魔盒，行业壁垒逐步瓦解》的爽文（对！爽"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://gme-hong.github.io/2023/10/06/ChatGPT/10e55600dccd279069d0b58207d487c0.jpeg"><meta property="og:image" content="https://gme-hong.github.io/2023/10/06/ChatGPT/dcffceea100ade33ea53501be945d8ce.jpeg"><meta property="og:image" content="https://gme-hong.github.io/2023/10/06/ChatGPT/0c2fb894842b39f67ecc33c9288dbd52.jpeg"><meta property="og:image" content="https://gme-hong.github.io/2023/10/06/ChatGPT/40c8529d16291e27f2a83f703b82c16b.png"><meta property="og:image" content="https://gme-hong.github.io/2023/10/06/ChatGPT/f6781e42fff838887dac39c51faa93c4.png"><meta property="og:image" content="https://gme-hong.github.io/2023/10/06/ChatGPT/af78a744ecc9dcf2b84d5a3e2ab46078.png"><meta property="og:image" content="https://gme-hong.github.io/2023/10/06/ChatGPT/55e5bf98008def23eef58ba85ae0a828.png"><meta property="og:image" content="https://gme-hong.github.io/2023/10/06/ChatGPT/cc640438c277e692b60b5e6996600585.jpeg"><meta property="og:image" content="https://gme-hong.github.io/2023/10/06/ChatGPT/618ca0f94d2d55e0bec876e5c4f2604f.png"><meta property="og:image" content="https://gme-hong.github.io/2023/10/06/ChatGPT/bd5342b6599340434ac528d0be5f58d1.png"><meta property="article:published_time" content="2023-10-06T14:43:39.000Z"><meta property="article:modified_time" content="2025-02-28T14:23:05.332Z"><meta property="article:author" content="OpticHong"><meta property="article:tag" content="AI"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://gme-hong.github.io/2023/10/06/ChatGPT/10e55600dccd279069d0b58207d487c0.jpeg"><link rel="icon" type="image/png" href="/images/favicon.ico" sizes="192x192"><link rel="apple-touch-icon" sizes="180x180" href="/images/favicon.ico"><meta name="theme-color" content="#A31F34"><link rel="shortcut icon" href="/images/favicon.ico"><title>ChatGPT 打开潘多拉魔盒，行业壁垒逐步瓦解 - OpticHong</title><link rel="stylesheet" href="https://registry.npmmirror.com/hexo-theme-redefine/2.7.1/files/source/fonts/Chillax/chillax.css"><link rel="stylesheet" href="https://gme-hong.github.io/notes/ravel-light.css"><link rel="stylesheet" type="text/css" href="//at.alicdn.com/t/c/font_4973007_vi96klaq9o8.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="https://registry.npmmirror.com/hexo-theme-redefine/2.7.1/files/source/assets/build/styles.css"><link rel="stylesheet" href="https://registry.npmmirror.com/hexo-theme-redefine/2.7.1/files/source/fonts/GeistMono/geist-mono.css"><link rel="stylesheet" href="https://registry.npmmirror.com/hexo-theme-redefine/2.7.1/files/source/fonts/Geist/geist.css"><link href="https://fonts.googleapis.com/css2?family=Tilt+Neon&display=swap" rel="stylesheet"><link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@200..900&display=swap" rel="stylesheet"><link href="https://fonts.googleapis.com/css2?family=Alice&display=swap" rel="stylesheet"><link href="https://fonts.googleapis.com/css2?family=Alice&display=swap" rel="stylesheet"><script id="hexo-configurations">window.config={hostname:"gme-hong.github.io",root:"/",language:"en",path:"search.json"},window.theme={articles:{style:{font_size:"16px",line_height:1.5,image_border_radius:"4px",image_alignment:"center",image_caption:!1,link_icon:!0,title_alignment:"left",headings_top_spacing:{h1:"3rem",h2:"2.5rem",h3:"2.2rem",h4:"1.8rem",h5:"1.4rem",h6:"1.0rem"}},word_count:{enable:!0,count:!0,min2read:!0},author_label:{enable:!0,auto:!1,list:["Lv.8"]},code_block:{copy:!0,style:"mac",font:{enable:!1,family:"Noto Sans",url:"https://fonts.googleapis.com/css2?family=Noto+Sans:ital,wght@0,100..900;1,100..900&display=swap"}},toc:{enable:!0,max_depth:6,number:!0,expand:!0,init_open:!0},copyright:{enable:!0,default:"cc_by_nc_sa"},lazyload:!0,recommendation:{enable:!0,title:"Recommendation",limit:3,mobile_limit:2,placeholder:"/images/background-light.jpg",skip_dirs:[]}},colors:{primary:"#A31F34",secondary:null,default_mode:"dark"},global:{fonts:{chinese:{enable:!0,family:"Noto Serif Simplified Chinese",url:"https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@200..900&display=swap"},english:{enable:!0,family:"Alice",url:"https://fonts.googleapis.com/css2?family=Alice&display=swap"},title:{enable:!0,family:"Alice",url:"https://fonts.googleapis.com/css2?family=Alice&display=swap"}},content_max_width:"1000px",sidebar_width:"210px",hover:{shadow:!0,scale:!1},scroll_progress:{bar:!1,percentage:!0},website_counter:{url:"https://cn.vercount.one/js",enable:!0,site_pv:!0,site_uv:!0,post_pv:!0},single_page:!0,preloader:{enable:!1,custom_message:null},open_graph:!0,google_analytics:{enable:!1,id:null}},home_banner:{enable:!0,style:"fixed",image:{light:"/images/background-light.jpg",dark:"/images/background-dark.jpg"},title:"Hong Nie's Blog",subtitle:{text:["莫愁前路无知己，天下谁人不识君。"],hitokoto:{enable:!1,api:"https://v1.hitokoto.cn"},typing_speed:100,backing_speed:60,starting_delay:500,backing_delay:1500,loop:!0,smart_backspace:!0},text_color:{light:"#fff",dark:"#d1d1b6"},text_style:{title_size:"2.8rem",subtitle_size:"1.5rem",line_height:1.2},custom_font:{enable:!0,family:"Tilt Neon",url:"https://fonts.googleapis.com/css2?family=Tilt+Neon&display=swap"},social_links:{enable:!0,style:"default",links:{github:"https://github.com/gme-hong","fa-brands fa-git-alt":"https://gitee.com/optichong",instagram:"https://www.instagram.com/optichong/",twitter:"https://twitter.com/OpticHong",zhihu:"https://www.zhihu.com/people/OpticHong",email:"info@optichong.cn","fa-solid fa-square-rss":["https://gme-hong.github.io/atom.xml"]},qrs:{weixin:"/images/sponsors.jpg"}}},plugins:{feed:{enable:!1,type:"atom",path:"atom.xml",limit:20,hub:null,content:null,content_limit:140,content_limit_delim:" ",order_by:"-date",icon:"/images/favicon.jpg",autodiscovery:!0,template:null},aplayer:{enable:!1,type:"fixed",audios:[{name:null,artist:null,url:null,cover:null,lrc:null}]},mermaid:{enable:!0,version:"9.3.0"},mathjax:{tags:"none",single_dollars:!0,cjk_width:.9,normal_width:.6,append_css:!0,every_page:!1}},version:"2.7.1",navbar:{auto_hide:!1,color:{left:"#f78736",right:"#367df7",transparency:35},width:{home:"1200px",pages:"1000px"},links:{Home:{path:"/",icon:"fa-solid fa-house"},Archives:{path:"/archives/",icon:"fa-solid fa-inbox"},Essays:{path:"/essays/",icon:"fa-solid fa-messages"},Papers:{path:"/papers/",icon:"fa-solid fa-newspaper"},About:{icon:"fa-solid fa-user",submenus:{Me:"/about/",Github:"https://github.com/gme-hong",Gallery:"/masonry/"}}},search:{enable:!0,preload:!0,icon:"fa-solid fa-magnifying-glass"}},page_templates:{friends_column:2,tags_style:"blur"},home:{sidebar:{enable:!0,position:"left",first_item:"menu",announcement:null,show_on_mobile:!0,links:{Tags:{path:"/tags/",icon:"fa-regular fa-tags"},Papers:{path:"/papers/",icon:"fa-solid fa-newspaper"},Archives:{path:"/archives/",icon:"fa-solid fa-inbox"},Categories:{path:"/categories/",icon:"fa-solid fa-layer-group"}}},article_date_format:"auto",categories:{enable:!0,limit:6},tags:{enable:!0,limit:5}},footerStart:"2022/3/10 00:00:00"},window.lang_ago={second:"%s seconds ago",minute:"%s minutes ago",hour:"%s hours ago",day:"%s days ago",week:"%s weeks ago",month:"%s months ago",year:"%s years ago"},window.data={masonry:!0}</script><link rel="stylesheet" href="https://registry.npmmirror.com/hexo-theme-redefine/2.7.1/files/source/fontawesome/fontawesome.min.css"><link rel="stylesheet" href="https://registry.npmmirror.com/hexo-theme-redefine/2.7.1/files/source/fontawesome/brands.min.css"><link rel="stylesheet" href="https://registry.npmmirror.com/hexo-theme-redefine/2.7.1/files/source/fontawesome/solid.min.css"><link rel="stylesheet" href="https://registry.npmmirror.com/hexo-theme-redefine/2.7.1/files/source/fontawesome/regular.min.css"><meta name="generator" content="Hexo 7.1.1"><style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style><link rel="alternate" href="/atom.xml" title="OpticHong's Blog" type="application/atom+xml">
</head><body><div class="progress-bar-container"><span class="pjax-progress-bar"></span></div><main class="page-container" id="swup"><div class="main-content-container"><div class="main-content-header"><header class="navbar-container px-6 md:px-12"><div class="navbar-content"><div class="left"><a class="logo-image" href="/"><img src="/images/favicon.jpg"> </a><a class="logo-title" href="/">OpticHong</a></div><div class="right"><div class="desktop"><ul class="navbar-list"><li class="navbar-item"><a href="/"><i class="fa-solid fa-house fa-fw"></i> HOME</a></li><li class="navbar-item"><a href="/archives/"><i class="fa-solid fa-inbox fa-fw"></i> ARCHIVES</a></li><li class="navbar-item"><a href="/essays/"><i class="fa-solid fa-messages fa-fw"></i> ESSAYS</a></li><li class="navbar-item"><a href="/papers/"><i class="fa-solid fa-newspaper fa-fw"></i> PAPERS</a></li><li class="navbar-item"><a class="has-dropdown" href="#" onclick="&#34;return" false;&#34;><i class="fa-solid fa-user fa-fw"></i> ABOUT <i class="fa-solid fa-chevron-down fa-fw"></i></a><ul class="sub-menu"><li><a href="/about/">ME</a></li><li><a target="_blank" rel="noopener" href="https://github.com/gme-hong">GITHUB</a></li><li><a href="/masonry/">GALLERY</a></li></ul></li><li class="navbar-item search search-popup-trigger"><i class="fa-solid fa-magnifying-glass"></i></li></ul></div><div class="mobile"><div class="icon-item search search-popup-trigger"><i class="fa-solid fa-magnifying-glass"></i></div><div class="icon-item navbar-bar"><div class="navbar-bar-middle"></div></div></div></div></div><div class="navbar-drawer h-screen w-full absolute top-0 left-0 bg-background-color flex flex-col justify-between"><ul class="drawer-navbar-list flex flex-col px-4 justify-center items-start"><li class="drawer-navbar-item text-base my-1.5 flex flex-col w-full"><a class="py-1.5 px-2 flex flex-row items-center justify-between gap-1 hover:!text-primary active:!text-primary text-2xl font-semibold group border-b border-border-color hover:border-primary w-full" href="/"><span>HOME </span><i class="fa-solid fa-house fa-sm fa-fw"></i></a></li><li class="drawer-navbar-item text-base my-1.5 flex flex-col w-full"><a class="py-1.5 px-2 flex flex-row items-center justify-between gap-1 hover:!text-primary active:!text-primary text-2xl font-semibold group border-b border-border-color hover:border-primary w-full" href="/archives/"><span>ARCHIVES </span><i class="fa-solid fa-inbox fa-sm fa-fw"></i></a></li><li class="drawer-navbar-item text-base my-1.5 flex flex-col w-full"><a class="py-1.5 px-2 flex flex-row items-center justify-between gap-1 hover:!text-primary active:!text-primary text-2xl font-semibold group border-b border-border-color hover:border-primary w-full" href="/essays/"><span>ESSAYS </span><i class="fa-solid fa-messages fa-sm fa-fw"></i></a></li><li class="drawer-navbar-item text-base my-1.5 flex flex-col w-full"><a class="py-1.5 px-2 flex flex-row items-center justify-between gap-1 hover:!text-primary active:!text-primary text-2xl font-semibold group border-b border-border-color hover:border-primary w-full" href="/papers/"><span>PAPERS </span><i class="fa-solid fa-newspaper fa-sm fa-fw"></i></a></li><li class="drawer-navbar-item-sub text-base my-1.5 flex flex-col w-full"><div class="py-1.5 px-2 flex flex-row items-center justify-between gap-1 hover:!text-primary active:!text-primary cursor-pointer text-2xl font-semibold group border-b border-border-color hover:border-primary w-full" navbar-data-toggle="submenu-About"><span>ABOUT </span><i class="fa-solid fa-chevron-right fa-sm fa-fw transition-all"></i></div><div class="flex-col items-start px-2 py-2 hidden" data-target="submenu-About"><div class="drawer-navbar-item text-base flex flex-col justify-center items-start hover:underline active:underline hover:underline-offset-1 rounded-3xl"><a class="text-third-text-color text-xl" href="/about/">ME</a></div><div class="drawer-navbar-item text-base flex flex-col justify-center items-start hover:underline active:underline hover:underline-offset-1 rounded-3xl"><a class="text-third-text-color text-xl" target="_blank" rel="noopener" href="https://github.com/gme-hong">GITHUB</a></div><div class="drawer-navbar-item text-base flex flex-col justify-center items-start hover:underline active:underline hover:underline-offset-1 rounded-3xl"><a class="text-third-text-color text-xl" href="/masonry/">GALLERY</a></div></div></li><li class="drawer-navbar-item text-base my-1.5 flex flex-col w-full"><a class="py-1.5 px-2 flex flex-row items-center justify-between gap-1 hover:!text-primary active:!text-primary text-2xl font-semibold group border-b border-border-color hover:border-primary w-full active" href="/tags/"><span>Tags</span> <i class="fa-regular fa-tags fa-sm fa-fw"></i></a></li><li class="drawer-navbar-item text-base my-1.5 flex flex-col w-full"><a class="py-1.5 px-2 flex flex-row items-center justify-between gap-1 hover:!text-primary active:!text-primary text-2xl font-semibold group border-b border-border-color hover:border-primary w-full active" href="/categories/"><span>Categories</span> <i class="fa-solid fa-layer-group fa-sm fa-fw"></i></a></li></ul><div class="statistics flex justify-around my-2.5"><a class="item tag-count-item flex flex-col justify-center items-center w-20" href="/tags"><div class="number text-2xl sm:text-xl text-second-text-color font-semibold">6</div><div class="label text-third-text-color text-sm">Tags</div></a><a class="item tag-count-item flex flex-col justify-center items-center w-20" href="/categories"><div class="number text-2xl sm:text-xl text-second-text-color font-semibold">2</div><div class="label text-third-text-color text-sm">Categories</div></a><a class="item tag-count-item flex flex-col justify-center items-center w-20" href="/archives"><div class="number text-2xl sm:text-xl text-second-text-color font-semibold">11</div><div class="label text-third-text-color text-sm">Posts</div></a></div></div><div class="window-mask"></div></header></div><div class="main-content-body"><div class="main-content"><div class="post-page-container flex relative justify-between box-border w-full h-full"><div class="article-content-container"><div class="article-title relative w-full"><div class="w-full flex items-center pt-6 justify-start"><h1 class="article-title-regular text-second-text-color tracking-tight text-4xl md:text-6xl font-semibold px-2 sm:px-6 md:px-8 py-3">ChatGPT 打开潘多拉魔盒，行业壁垒逐步瓦解</h1></div></div><div class="article-header flex flex-row gap-2 items-center px-2 sm:px-6 md:px-8"><div class="avatar w-[46px] h-[46px] flex-shrink-0 rounded-medium border border-border-color p-[1px]"><img src="/images/favicon.jpg"></div><div class="info flex flex-col justify-between"><div class="author flex items-center"><span class="name text-default-text-color text-lg font-semibold">Hong Nie</span> <span class="author-label ml-1.5 text-xs px-2 py-0.5 rounded-small text-third-text-color border border-shadow-color-1">Lv.8</span></div><div class="meta-info"><div class="article-meta-info"><span class="article-date article-meta-item"><i class="fa-regular fa-pen-fancy"></i>&nbsp; <span class="desktop">2023-10-06 22:43:39</span> <span class="mobile">2023-10-06 22:43:39</span> <span class="hover-info">Created</span> </span><span class="article-date article-meta-item"><i class="fa-regular fa-wrench"></i>&nbsp; <span class="desktop">2025-02-28 22:23:05</span> <span class="mobile">2025-02-28 22:23:05</span> <span class="hover-info">Updated</span> </span><span class="article-categories article-meta-item"><i class="fa-regular fa-folders"></i>&nbsp;<ul><li><a href="/categories/Notes/">Notes</a>&nbsp;</li></ul></span><span class="article-tags article-meta-item"><i class="fa-regular fa-tags"></i>&nbsp;<ul><li><a href="/tags/AI/">AI</a>&nbsp;</li></ul></span><span class="article-wordcount article-meta-item"><i class="fa-regular fa-typewriter"></i>&nbsp;<span>6.4k Words</span> </span><span class="article-min2read article-meta-item"><i class="fa-regular fa-clock"></i>&nbsp;<span>22 Mins</span> </span><span class="article-pv article-meta-item"><i class="fa-regular fa-eye"></i>&nbsp;<span id="busuanzi_value_page_pv"></span></span></div></div></div></div><div class="article-content markdown-body px-2 sm:px-6 md:px-8 pb-8"><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>作为一名非专精于NLP算法的同学，去详细论述ChatGPT的底层原理是很困难的。但好在最近为了搭建一个轻量化的RGM(Robotic Grasp Model)，接触到了一部分的图像注意力机制。想着CV、NLP本是一家亲，更何况ChatGPT的前世还可以追溯到《Attention is all you need》，为此斗胆写了一篇《ChatGPT 打开潘多拉魔盒，行业壁垒逐步瓦解》的爽文（对！爽文，让你轻松拿捏这个庞然大物），仅供同学们交流学习！</p><p>在本文中你将会了解到一些人工智能的前置知识、ChatGPT的概念、Transformer的基本原理、还有我关于ChatGPT的见解~</p><h2 id="人工智能的前置知识"><a href="#人工智能的前置知识" class="headerlink" title="人工智能的前置知识"></a>人工智能的前置知识</h2><p>ChatGPT作为一个人工智能LLM (Large Language Model) 大语言模型，细盘之前，我们先了解一下一些基本的人工智能知识。</p><h3 id="机器学习"><a href="#机器学习" class="headerlink" title="机器学习"></a>机器学习</h3><p>机器学习，作为计算机专业研一同学必修的一门课程，足以体现其在人工智能领域举足轻重的地位。虽然现在学术前沿领域都在深度学习这条赛道上一发不可收拾，但是搞深度学习前还是得了解一些机器学习哦。<code>smooth_l1_loss</code>、<code>sigmoid</code>都会吧，同学？这些在用<code>pytorch</code>搞深度学习时常用的损失函数、激活函数可都是机器学习的知识哦。撇开HMM(Hidden Markov Model)、CRF(Conditional Random Field)等一些集成方法不谈，无论是决策树、K-means聚类、支持向量机、梯度提升树还是朴素贝叶斯，这些经典的机器学习模型也都是一些工业应用领域的常青树哦！（别问我为什么，去问度娘~）。</p><p>讲了一大堆机器学习的重要性，都还没说这到底是个啥。来，上概念。</p><p>机器学习(Machine Learning，ML)是指从有限的观测数据中学习(或“猜测”)出具有一般性的规律，并将这些规律应用到未观测数据样本上的方法。主要研究内容是学习算法。基本流程是基于数据产生模型，利用模型预测输出。目标是让模型有较好泛化能力。</p><p>什么？太抽象了，不好理解？</p><p>举一个经典的例子，我们挑西瓜的时候是如何判断一个西瓜是否成熟的呢？每个人一开始都是不会挑选的，但是随着我们耳濡目染，看了很多挑西瓜能手是怎么做的，发现可以通过西瓜的颜色，大小，产地，纹路，敲击声等等因素来判断，那么这个就是一个学习的过程。</p><p>那么人工智能、机器学习以及深度学习的关系是什么呢？或者说这个家族的族谱应该怎么画呢？</p><p>别急，一张图搞明白他们的关系。</p><style>.fpxhvqfvzevl{zoom:50%}</style><img lazyload src="/images/loading.svg" data-src="/2023/10/06/ChatGPT/10e55600dccd279069d0b58207d487c0.jpeg" class="fpxhvqfvzevl" alt="10e55600dccd279069d0b58207d487c0.jpeg"><p>不知道你有没发现，这些方法的进化过程就是人类专精于摸鱼的过程。从以前纯人工作业到现在半人工、甚至全智能作业，从需要繁琐的数据预处理和客制化的特征提取手段的机器学习过渡到让机器自己去学特征的深度学习，从有监督任务过渡到自监督任务、多阶段深度学习算法过渡到端到端的算法，从手动搭建合适的深度网络模型到NAS(Neural Architecture Search) ……</p><p>哇，果然人类摸鱼的潜能是无限的~</p><h3 id="参数-权重："><a href="#参数-权重：" class="headerlink" title="参数 / 权重："></a>参数 / 权重：</h3><p>前文我们说过，ChatGPT是一个LLM。对于任何模型而言，无非就两种表示形式，即<code>y=f(x)</code>型和<code>P(y|x)</code>型。前者是确定性模型(也称非概率模型)，后者是概率模型。无非是哪一种，其中不可或缺的就是参数。</p><p>我们举几个例子说明参数的重要性。</p><p>如<code>y=f(x)</code>模型中最简单的y=wx+b。先不引入任何机器学习的概念，这就是我们小学学的二元一次方程，只不过我们当时w和b往往是给定的，我们最关注的就是如何解得x和y。但在机器学习中，往往x是给定的，需要我们求解出(拟合出)w和b的值。按照我们小学课本里教的，w就是斜率，b就是截距。说到这里不知道你意识到没，这就是知识啊！我们知道了其特定的含义就能在一个二维平面中可视化这条直线，机器知道了就能做一个简单的yes和no的回答。又如<code>P(y|x)</code>模型中的<br><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align:-.566ex" xmlns="http://www.w3.org/2000/svg" width="20.88ex" height="2.262ex" role="img" focusable="false" viewbox="0 -750 9228.9 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D43B" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 219 683Q260 681 355 681Q389 681 418 681T463 682T483 682Q499 682 499 672Q499 670 497 658Q492 641 487 638H485Q483 638 480 638T473 638T464 637T455 637Q416 636 405 634T387 623Q384 619 355 500Q348 474 340 442T328 395L324 380Q324 378 469 378H614L615 381Q615 384 646 504Q674 619 674 627T617 637Q594 637 587 639T580 648Q580 650 582 660Q586 677 588 679T604 682Q609 682 646 681T740 680Q802 680 835 681T871 682Q888 682 888 672Q888 645 876 638H874Q872 638 869 638T862 638T853 637T844 637Q805 636 794 634T776 623Q773 618 704 340T634 58Q634 51 638 51Q646 48 692 46H723Q729 38 729 37T726 19Q722 6 716 0H701Q664 2 567 2Q533 2 504 2T458 2T437 1Q420 1 420 10Q420 15 423 24Q428 43 433 45Q437 46 448 46H454Q481 46 514 49Q520 50 522 50T528 55T534 64T540 82T547 110T558 153Q565 181 569 198Q602 330 602 331T457 332H312L279 197Q245 63 245 58Q245 51 253 49T303 46H334Q340 38 340 37T337 19Q333 6 327 0H312Q275 2 178 2Q144 2 115 2T69 2T48 1Q31 1 31 10Q31 12 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"/></g><g data-mml-node="mi" transform="translate(888,0)"><path data-c="1D440" d="M289 629Q289 635 232 637Q208 637 201 638T194 648Q194 649 196 659Q197 662 198 666T199 671T201 676T203 679T207 681T212 683T220 683T232 684Q238 684 262 684T307 683Q386 683 398 683T414 678Q415 674 451 396L487 117L510 154Q534 190 574 254T662 394Q837 673 839 675Q840 676 842 678T846 681L852 683H948Q965 683 988 683T1017 684Q1051 684 1051 673Q1051 668 1048 656T1045 643Q1041 637 1008 637Q968 636 957 634T939 623Q936 618 867 340T797 59Q797 55 798 54T805 50T822 48T855 46H886Q892 37 892 35Q892 19 885 5Q880 0 869 0Q864 0 828 1T736 2Q675 2 644 2T609 1Q592 1 592 11Q592 13 594 25Q598 41 602 43T625 46Q652 46 685 49Q699 52 704 61Q706 65 742 207T813 490T848 631L654 322Q458 10 453 5Q451 4 449 3Q444 0 433 0Q418 0 415 7Q413 11 374 317L335 624L267 354Q200 88 200 79Q206 46 272 46H282Q288 41 289 37T286 19Q282 3 278 1Q274 0 267 0Q265 0 255 0T221 1T157 2Q127 2 95 1T58 0Q43 0 39 2T35 11Q35 13 38 25T43 40Q45 46 65 46Q135 46 154 86Q158 92 223 354T289 629Z"/></g><g data-mml-node="mi" transform="translate(1939,0)"><path data-c="1D440" d="M289 629Q289 635 232 637Q208 637 201 638T194 648Q194 649 196 659Q197 662 198 666T199 671T201 676T203 679T207 681T212 683T220 683T232 684Q238 684 262 684T307 683Q386 683 398 683T414 678Q415 674 451 396L487 117L510 154Q534 190 574 254T662 394Q837 673 839 675Q840 676 842 678T846 681L852 683H948Q965 683 988 683T1017 684Q1051 684 1051 673Q1051 668 1048 656T1045 643Q1041 637 1008 637Q968 636 957 634T939 623Q936 618 867 340T797 59Q797 55 798 54T805 50T822 48T855 46H886Q892 37 892 35Q892 19 885 5Q880 0 869 0Q864 0 828 1T736 2Q675 2 644 2T609 1Q592 1 592 11Q592 13 594 25Q598 41 602 43T625 46Q652 46 685 49Q699 52 704 61Q706 65 742 207T813 490T848 631L654 322Q458 10 453 5Q451 4 449 3Q444 0 433 0Q418 0 415 7Q413 11 374 317L335 624L267 354Q200 88 200 79Q206 46 272 46H282Q288 41 289 37T286 19Q282 3 278 1Q274 0 267 0Q265 0 255 0T221 1T157 2Q127 2 95 1T58 0Q43 0 39 2T35 11Q35 13 38 25T43 40Q45 46 65 46Q135 46 154 86Q158 92 223 354T289 629Z"/></g><g data-mml-node="mo" transform="translate(2990,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/></g><g data-mml-node="mi" transform="translate(3379,0)"><path data-c="1D707" d="M58 -216Q44 -216 34 -208T23 -186Q23 -176 96 116T173 414Q186 442 219 442Q231 441 239 435T249 423T251 413Q251 401 220 279T187 142Q185 131 185 107V99Q185 26 252 26Q261 26 270 27T287 31T302 38T315 45T327 55T338 65T348 77T356 88T365 100L372 110L408 253Q444 395 448 404Q461 431 491 431Q504 431 512 424T523 412T525 402L449 84Q448 79 448 68Q448 43 455 35T476 26Q485 27 496 35Q517 55 537 131Q543 151 547 152Q549 153 557 153H561Q580 153 580 144Q580 138 575 117T555 63T523 13Q510 0 491 -8Q483 -10 467 -10Q446 -10 429 -4T402 11T385 29T376 44T374 51L368 45Q362 39 350 30T324 12T288 -4T246 -11Q199 -11 153 12L129 -85Q108 -167 104 -180T92 -202Q76 -216 58 -216Z"/></g><g data-mml-node="mo" transform="translate(4259.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"/></g><g data-mml-node="mo" transform="translate(5315.6,0)"><path data-c="5B" d="M118 -250V750H255V710H158V-210H255V-250H118Z"/></g><g data-mml-node="mi" transform="translate(5593.6,0)"><path data-c="1D70B" d="M132 -11Q98 -11 98 22V33L111 61Q186 219 220 334L228 358H196Q158 358 142 355T103 336Q92 329 81 318T62 297T53 285Q51 284 38 284Q19 284 19 294Q19 300 38 329T93 391T164 429Q171 431 389 431Q549 431 553 430Q573 423 573 402Q573 371 541 360Q535 358 472 358H408L405 341Q393 269 393 222Q393 170 402 129T421 65T431 37Q431 20 417 5T381 -10Q370 -10 363 -7T347 17T331 77Q330 86 330 121Q330 170 339 226T357 318T367 358H269L268 354Q268 351 249 275T206 114T175 17Q164 -11 132 -11Z"/></g><g data-mml-node="mo" transform="translate(6163.6,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"/></g><g data-mml-node="mi" transform="translate(6608.2,0)"><path data-c="1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z"/></g><g data-mml-node="mo" transform="translate(7358.2,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"/></g><g data-mml-node="mi" transform="translate(7802.9,0)"><path data-c="1D435" d="M231 637Q204 637 199 638T194 649Q194 676 205 682Q206 683 335 683Q594 683 608 681Q671 671 713 636T756 544Q756 480 698 429T565 360L555 357Q619 348 660 311T702 219Q702 146 630 78T453 1Q446 0 242 0Q42 0 39 2Q35 5 35 10Q35 17 37 24Q42 43 47 45Q51 46 62 46H68Q95 46 128 49Q142 52 147 61Q150 65 219 339T288 628Q288 635 231 637ZM649 544Q649 574 634 600T585 634Q578 636 493 637Q473 637 451 637T416 636H403Q388 635 384 626Q382 622 352 506Q352 503 351 500L320 374H401Q482 374 494 376Q554 386 601 434T649 544ZM595 229Q595 273 572 302T512 336Q506 337 429 337Q311 337 310 336Q310 334 293 263T258 122L240 52Q240 48 252 48T333 46Q422 46 429 47Q491 54 543 105T595 229Z"/></g><g data-mml-node="mo" transform="translate(8561.9,0)"><path data-c="5D" d="M22 710V750H159V-250H22V-210H119V710H22Z"/></g><g data-mml-node="mo" transform="translate(8839.9,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></g></g></g></svg></mjx-container><br>无论是初始概率分布，状态转移矩阵还是观测矩阵都是HMM主要的参数来源啊，而这三个部分也正是HMM的核心。</p><p>而我们所津津乐道的ChatGPT就是一个除了参数还是参数的大怪物，其拥有这可怕的1750亿的参数规模(本来还想说一下其所需的浮点运算次数的，emmm，找了半天没找到)。要训练一个千亿参数量级的模型，所需要支持的算力那也应该不是我们能设想的。再加上<code>human in the loop</code>的机制，哇，烧钱的嘞。</p><h3 id="过拟合、欠拟合与模型退化"><a href="#过拟合、欠拟合与模型退化" class="headerlink" title="过拟合、欠拟合与模型退化"></a>过拟合、欠拟合与模型退化</h3><p>如果你已经把所有的菜都备齐了，炒菜的设备也上齐了，那么接下来就应该是生火开炒了——这里具体如何炒，先加盐还是先放醋针对不同的菜应该有不同的策略，正如机器学习针对不同的任务需要设计不同的特征提取策略一样，为此这里我们还是用深度学习的黑箱模型来说事(其实主要是我不会炒菜……)。最理想的状态肯定就是炒完后菜的口感最能满足你挑剔的嘴，但是事情往往并不会向我们所期待的那样，炒焦了和没炒熟都是很常见的情况。模型的训练也正如这个做菜的过程，稍不留神就过拟、欠拟。但这还是好的，最崩溃的情况是你拿一只大锅发现怎么样也炒不出小锅的味道，而且每次用大锅颠勺都还让你累的够呛。这也就是模型的退化现象，好像这就是自然界的规律，过犹而不及。正如Google的PaLM(5400亿参数)，微软和英伟达合作的MT-NLG(5300亿参数)跟ChatGPT比起来都差点意思。</p><h3 id="人类反馈强化学习（RLHF）"><a href="#人类反馈强化学习（RLHF）" class="headerlink" title="人类反馈强化学习（RLHF）"></a>人类反馈强化学习（RLHF）</h3><p>人类反馈强化学习，一听到这个名词是不是就感觉特别高大尚。害其实也没什么，就是智能还不够智能，还需要人类从中插一脚。RLHF最本质的特点就是<code>human in the loop</code>，也就是人在回路。当然认真看过流浪地球2的同学们应该还是不太陌生的。</p><p>RLHF 主要包括三个步骤：</p><ol><li>使用监督学习训练语言模型；</li><li>根据人类偏好收集比较数据并训练奖励模型；</li><li><strong>使用强化学习针对奖励模型优化语言模型。</strong></li></ol><p>它使模型能够通过从人类获取反馈，从而不断改进自身学习技能，从而有效地适应实际环境。都说到这个份上了，养过猫猫狗狗的同学应该就能get到了，你驯化它的过程就是RLHF。</p><h3 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h3><p>神经网络，都以“神经”这两个字命名了，那肯定就和人的神经元系统相当类似咯。无论是你眼镜接收到的视觉信息、皮肤感知到的触觉信息还是耳朵获取的听觉信息都需要从感受器通过电信号(和化学信号)的形式传导到神经中枢，而这就跟输入向量在神经网络的传导过程十分类似。对于冲动传输链路上的每一个神经元扮演了一个节流阀的作用(激活还是抑制)，类似于神经网络的前向传播过程，每一层的每个节点都能决定通过其这条沿线的权重大小。</p><style></style><img lazyload src="/images/loading.svg" data-src="/2023/10/06/ChatGPT/dcffceea100ade33ea53501be945d8ce.jpeg" class="dhxcosvrctlm"><style></style><img lazyload src="/images/loading.svg" data-src="/2023/10/06/ChatGPT/0c2fb894842b39f67ecc33c9288dbd52.jpeg" class="sczzsamhqdpy"><p>神经网络的结构正如图五所示，最基本形式的人工神经网络有三层神经元。信息从一层神经元流向另一层，就像在人脑中一样：</p><ol><li>输入层：数据进入系统的入口点</li><li>隐藏层：处理信息的地方</li><li>输出层：系统根据数据决定如何继续操作的位置</li></ol><h2 id="ChatGPT-的概念"><a href="#ChatGPT-的概念" class="headerlink" title="ChatGPT 的概念"></a>ChatGPT 的概念</h2><p>哇，终于写到正题了，eight hours later……</p><p>GPT 对应的是三个单词：Generative，Pre-Training，Transformer。</p><p><strong>Generative</strong>：生成式，比较好理解，通过学习历史数据，来生成<strong>全新</strong>的数据。请注意，这里是全新的哦！不过全新也要看你怎么理解，如果想说它能所回答你的每一个字是不是全新的，那肯定是不现实的。这种新应该是一种全新的、带有人类语言逻辑的语句组合。更直观地去理解Generative，那肯定就得提及DALL-E(emmm，这家伙也是OpenAI的)，这个东西能够根据一段场景描述性的文字来生成一张与之对应的、全新的图片(类似于上面对全新概念的理解，你总不能要0-255的像素值是全新的吧)。从这个角度去理解ChatGPT，那它就一个词生成器啊。例如ChatGPT已经生成了“今天晚上我们去打”，那么下一个生成的字/词就有可能是“台球”、“篮球”、“羽毛球”、“乒乓球”、“游戏”，它的生成逻辑就是从这些候选的单元中选出一个概率最大值。如果你使用过ChatGPT，并且详细观察过它回答你的方式，你可能会对「逐字」这个概念有更深的感触。</p><p><strong>Pre-Training</strong>：预训练，顾名思义就是预先训练的意思，说白了就是一个特征提取的过程。“预训练“方法的诞生是出于这样的现实：标注资源稀缺而无标注资源丰富(某种特殊的任务只存在非常少量的相关训练数据，以至于模型不能从中学习总结到有用的规律)。</p><p>举个简单的例子，现在有一个对英语一窍不通的同学和一个英语基础尚佳的同学同时去完成翻译并总结一篇英语技术文章的任务。对前者来说就需要先学会英文 26 个字母，进而学会单词语法等，再去了解这篇文章相关的技术，最后才能去完成我们指派的任务。但是对后者来做这个任务就相对简单的多，他只需要去大致了解一下这篇文章所涉及到的技术，便能很好的总结出来。</p><p>这就是预训练，<strong>先把一些通用能力提前训练出来</strong>。人工智能本身就是一个不断训练参数的过程，如果我们可以提前把通用能力相关的参数提前训练好，那么在一些特殊的场景，发现通用能力不能完全适配时，只做简单的参数微调即可，这样做大幅减少了每个独立训练预测任务的计算成本。反映在具体模型上就是：</p><ol><li>模型参数不再是随机初始化，而是通过一些任务(如语言模型)进行预训练</li><li>将训练任务拆解成共性学习和特性学习两个步骤</li></ol><p><strong>Transformer</strong>：这是 ChatGPT 的灵魂，它是一个神经网络架构。后文再进行详细的说明。</p><p>以上就是 ChatGPT 的基本概念，结合起来就是一个采用了预训练和强化学习策略的生成式神经网络模型，它能够对人类的对话进行模拟。</p><h2 id="Transformer基本知识"><a href="#Transformer基本知识" class="headerlink" title="Transformer基本知识"></a>Transformer基本知识</h2><p>因为介绍Transformer的文章很多啊，在此我就直接搬运王正学长在《<a class="link" target="_blank" rel="noopener" href="https://juejin.cn/post/7277802797473841186">让非算法同学也能了解 ChatGPT 等相关大模型 <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a>》文章中论述的了。别看Transformer(变形金刚，很漫威)这个名字很玄乎，其实大致也就三个主要部分：Embedding、Self-Attention以及Softmax。</p><p>Embedding很好理解，就是要把输入的自然语言转换成机器能理解的向量表示。当然为了充分利用语言的序列特性，因此还需要加入额外的位置编码。</p><p>Self-Attention也就是我们常说的自注意力机制，emmm有点复杂，好像一句话不怎么讲的清楚，其实也就是<code>"What do I care most about myself"</code>。</p><p>Softmax这个好理解，直接上公式：<br><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align:-2.27ex" xmlns="http://www.w3.org/2000/svg" width="27.204ex" height="5.573ex" role="img" focusable="false" viewbox="0 -1460 12024.1 2463.1"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D446" d="M308 24Q367 24 416 76T466 197Q466 260 414 284Q308 311 278 321T236 341Q176 383 176 462Q176 523 208 573T273 648Q302 673 343 688T407 704H418H425Q521 704 564 640Q565 640 577 653T603 682T623 704Q624 704 627 704T632 705Q645 705 645 698T617 577T585 459T569 456Q549 456 549 465Q549 471 550 475Q550 478 551 494T553 520Q553 554 544 579T526 616T501 641Q465 662 419 662Q362 662 313 616T263 510Q263 480 278 458T319 427Q323 425 389 408T456 390Q490 379 522 342T554 242Q554 216 546 186Q541 164 528 137T492 78T426 18T332 -20Q320 -22 298 -22Q199 -22 144 33L134 44L106 13Q83 -14 78 -18T65 -22Q52 -22 52 -14Q52 -11 110 221Q112 227 130 227H143Q149 221 149 216Q149 214 148 207T144 186T142 153Q144 114 160 87T203 47T255 29T308 24Z"/></g><g data-mml-node="mi" transform="translate(645,0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"/></g><g data-mml-node="mi" transform="translate(1130,0)"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z"/></g><g data-mml-node="mi" transform="translate(1680,0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"/></g><g data-mml-node="mi" transform="translate(2041,0)"><path data-c="1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z"/></g><g data-mml-node="mi" transform="translate(2919,0)"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"/></g><g data-mml-node="mi" transform="translate(3448,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"/></g><g data-mml-node="mo" transform="translate(4020,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/></g><g data-mml-node="msub" transform="translate(4409,0)"><g data-mml-node="mi"><path data-c="1D467" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z"/></g><g data-mml-node="mi" transform="translate(498,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"/></g></g><g data-mml-node="mo" transform="translate(5478.7,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"/></g><g data-mml-node="mfrac" transform="translate(6534.5,0)"><g data-mml-node="mrow" transform="translate(994.8,710)"><g data-mml-node="mi"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"/></g><g data-mml-node="mi" transform="translate(466,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"/></g><g data-mml-node="mi" transform="translate(1038,0)"><path data-c="1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"/></g><g data-mml-node="mo" transform="translate(1541,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/></g><g data-mml-node="msub" transform="translate(1930,0)"><g data-mml-node="mi"><path data-c="1D467" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z"/></g><g data-mml-node="mi" transform="translate(498,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"/></g></g><g data-mml-node="mo" transform="translate(2722,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></g></g><g data-mml-node="mrow" transform="translate(220,-710)"><g data-mml-node="munder"><g data-mml-node="mo"><path data-c="2211" d="M61 748Q64 750 489 750H913L954 640Q965 609 976 579T993 533T999 516H979L959 517Q936 579 886 621T777 682Q724 700 655 705T436 710H319Q183 710 183 709Q186 706 348 484T511 259Q517 250 513 244L490 216Q466 188 420 134T330 27L149 -187Q149 -188 362 -188Q388 -188 436 -188T506 -189Q679 -189 778 -162T936 -43Q946 -27 959 6H999L913 -249L489 -250Q65 -250 62 -248Q56 -246 56 -239Q56 -234 118 -161Q186 -81 245 -11L428 206Q428 207 242 462L57 717L56 728Q56 744 61 748Z"/></g><g data-mml-node="mi" transform="translate(1089,-285.4) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"/></g></g><g data-mml-node="mi" transform="translate(1549.6,0)"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"/></g><g data-mml-node="mi" transform="translate(2015.6,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"/></g><g data-mml-node="mi" transform="translate(2587.6,0)"><path data-c="1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"/></g><g data-mml-node="mo" transform="translate(3090.6,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/></g><g data-mml-node="msub" transform="translate(3479.6,0)"><g data-mml-node="mi"><path data-c="1D467" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z"/></g><g data-mml-node="mi" transform="translate(498,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"/></g></g><g data-mml-node="mo" transform="translate(4271.6,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></g></g><rect width="4860.6" height="60" x="120" y="220"/></g><g data-mml-node="mo" transform="translate(11635.1,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></g></g></g></svg></mjx-container><br>这东西的本质就像我上面举的“今天晚上我们去打”的例子，就是选出一个最大概率呗。</p><hr><h3 id="第一步：embedding"><a href="#第一步：embedding" class="headerlink" title="第一步：embedding"></a>第一步：embedding</h3><p>embedding 的过程可以简单的理解为向量化。因为输入是一个个的词(token)，那需要把它映射成一个向量，embedding就是给定任何一个词，用一个向量来表示它。</p><p>在 embedding 的过程中，每个 token 都用一个单层神经网络转化为长度为 768(对于GPT-2)或 12288(对于ChatGPT的GPT-3)的 embedding 向量。</p><p>同时，模块中还有一个“辅助通路”(secondary pathway)，用于将 token 的整数位置转化为 embedding 向量。最后，将 token 值和 token 位置的 embedding 向量加在一起，生成最终的 embedding 向量序列。</p><p>那么为什么要将 token 值和 token 位置的 embedding 向量相加呢？只是尝试了各种不同的方法后发现这种方法似乎可行，而且神经网络的本身也认为，只要初始设置“大致正确”，通过足够的训练，通常算法可以自动调整细节。</p><p>以字符串“湖人”为例，在 gpt-2 中它可以将其转化为一系列长度为 768 的 embedding 向量，其中包括从每个 token 的值和位置中提取的信息。</p><style>.hanwsgrrylud{zoom:50%}</style><img lazyload src="/images/loading.svg" data-src="/2023/10/06/ChatGPT/40c8529d16291e27f2a83f703b82c16b.png" class="hanwsgrrylud" alt="40c8529d16291e27f2a83f703b82c16b.png"><p>第一张图中就是 token embedding，纵向一列代表一个向量，可以看到最先排列的是“湖”所代表的向量，然后是“人”所代表的向量。第二张图是位置的 embedding，代表着这两个字的位置信息。将这两两个 embedding 相加得到了最终的 embedding 序列。</p><h3 id="第二步：Attention"><a href="#第二步：Attention" class="headerlink" title="第二步：Attention"></a>第二步：Attention</h3><p>Attention 是整个 transformer 的主要部分，其内部结构是非常复杂的，我作为一名非专业人士，无法面面俱到的将其中的细节完全解释清楚，因此只能把它的核心能力简单叙述一二。</p><p>在进行 embedding 之后，需要对一系列的“注意力块”进行数据操作(gpt3 中有 96 个注意力块)，而每个“注意力块”中都有一组 attention heads，每个 attention head 都独立地作用于 embedding 向量中不同值的块。</p><p>attention head 的作用就是对历史的 token 序列进行回顾，这里的历史 token 序列就是已经生成的文本，之后将这些信息进行打包，以便找到下一个 token。稍微具体的来说，attention head 的作用是重新组合与不同 token 相关的 embedding 向量的块，并赋予一定的权重。</p><p><strong>举个例子</strong>：</p><p>用 ChatGPT 翻译句子“蚂蚁集团”到“ant group”举例，首先进行上一步 embedding 操作，将句子向量化并吸收句子位置信息，得到一个句子的初始向量组。</p><img lazyload src="/images/loading.svg" data-src="/2023/10/06/ChatGPT/f6781e42fff838887dac39c51faa93c4.png" title="f6781e42fff838887dac39c51faa93c4.png"><p>由于样本每个句子长短不同，所以每个句子都会是一个 512 x 512 的矩阵，如果长度不够就用 0 来代替。这样在训练时，无论多长的句子，都可以用一个同样规模的矩阵来表示。当然 512 是超参，可以在训练前调整大小。</p><p>接着，用每个字的初始向量分别乘以三个随机初始的矩阵W^Q,W^K,W^V分别得到三个量Qx，Kx，Vx，这样就得到了三个量：Qx，Kx，Vx，比如用“蚂”这个字举例：</p><style>.onzmmgqtmtcn{zoom:50%}</style><img lazyload src="/images/loading.svg" data-src="/2023/10/06/ChatGPT/af78a744ecc9dcf2b84d5a3e2ab46078.png" class="onzmmgqtmtcn" alt="af78a744ecc9dcf2b84d5a3e2ab46078.png"><p>然后，计算每个单词的 Attention 数值，比如“蚂”字的 Attention 值就是用“蚂”字的 Q蚂Q蚂 分别乘以句子中其他单词的 K 值，两个矩阵相乘的数学含义就是衡量两个矩阵的相似度。</p><h3 id="第三步：将向量转为概率"><a href="#第三步：将向量转为概率" class="headerlink" title="第三步：将向量转为概率"></a>第三步：将向量转为概率</h3><p>继续用上面翻译的例子：用计算出的每个单词的 Attention 值，通过一个 SoftMax 转换(这里不必关注是怎么转换的)，计算出它跟每个单词的权重，这个权重比例所有加在一起要等于 1。再用每个权重乘以相对应的 V 值。所有乘积相加得到这个 Attention 值。</p><style>.wvbalnphputy{zoom:50%}</style><img lazyload src="/images/loading.svg" data-src="/2023/10/06/ChatGPT/55e5bf98008def23eef58ba85ae0a828.png" class="wvbalnphputy" alt="55e5bf98008def23eef58ba85ae0a828.png"><p>这个 Attention 数值就是除了“蚂”字自有信息和位置信息以外，成功的得到了这个句子中每个单词的相关度信息。</p><p>在计算 Attention 之后，每个单词根据语义关系被打入了新的高维空间，这就是 Self-Attention(自注意力机制)。</p><p>但在 transformer 里，并不是代入了一个空间，而是代入了多个高维空间，叫做 Multi-Head Attention (多头注意力机制)。将高维空间引入模型训练的主要原因是它在训练时表现出很好的效果，这是人工智能科研论文的一个常见特点，研究人员凭借着极高的科研素养和敏感性，发现一些方向，并通过测试证明其有效性，但不一定有完美的理论支持。这为后续研究者提供了进一步完善的余地。</p><p>事实证明，如何提升Attention(Q，K，V)效率是 transformer 领域迭代最快的部分。</p><p>这就是 transformer 的大致原理，有一些细节我个人也没有深入研究，有兴趣的人可以自行去搜索。</p><hr><p><strong>！！！开始夹带私货！！！</strong></p><p>正如王正学长文章中所提到的——“如何提升Attention(Q，K，V)效率是 transformer 领域迭代最快的部分”，我是真的深有同感！之前还没那么多，但真的就是自己开始去跑代码的时候就知道，Attention是有多慢！！！特别是，我做的轻量化网络，就是那种除Self-Attention之外的部分占三分之一，Self-Attention占三分之二，参数量如此，GFLOPs更是如此……</p><p>既然都说到这里了，那我们也来倒腾一下CV领域的Attention呗。</p><style>.napanhrjmlmo{zoom:50%}</style><img lazyload src="/images/loading.svg" data-src="/2023/10/06/ChatGPT/cc640438c277e692b60b5e6996600585.jpeg" class="napanhrjmlmo" alt="cc640438c277e692b60b5e6996600585.jpeg"><p>将Self-Attention机制引入CV领域相当重要的一篇文献就是《Non-local Neural Networks》。与对文本进行的Transformer类似，只不过由对文本的Embedding操作不再需要了，转而需要对完整的图像进行切块处理和对每一个小块进行位置编码(传统的CNN本来含有位置信息，因此没必要添加额外的位置信息)，接下来的操作和NLP中的Self-Attention大差不差了。</p><style>.tnmlnrnniobr{zoom:50%}</style><img lazyload src="/images/loading.svg" data-src="/2023/10/06/ChatGPT/618ca0f94d2d55e0bec876e5c4f2604f.png" class="tnmlnrnniobr" alt="618ca0f94d2d55e0bec876e5c4f2604f.png"><p>如图所示的Self-Attention，这可怕的参数量……</p><p>后面还有一篇《CCNet: Criss-Cross Attention for Semantic Segmentation》是对NLNet的优化。强烈推荐看一下<a class="link" target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/51393573">CCNet–于”阡陌交通”处超越恺明Non-local <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a>这篇博客，写的也是相当好(主要是标题挺吸引^_^)。</p><style>.fmogurfmtjoy{zoom:50%}</style><img lazyload src="/images/loading.svg" data-src="/2023/10/06/ChatGPT/bd5342b6599340434ac528d0be5f58d1.png" class="fmogurfmtjoy" alt="bd5342b6599340434ac528d0be5f58d1.png"><h2 id="为什么说ChatGPT打开了潘多拉魔盒"><a href="#为什么说ChatGPT打开了潘多拉魔盒" class="headerlink" title="为什么说ChatGPT打开了潘多拉魔盒"></a>为什么说ChatGPT打开了潘多拉魔盒</h2><p>前置内容说得太多了，终于开始说和标题有关的内容了，再拖拖都成标题党了。</p><p>不过在谈这个话题之前我还是想多说两句。要不是得完成课题作业，我也不至于议论这样的话题，确实以一个本科生的角度去观望整个行业确实有点管中窥豹。不过气氛都烘托到这了，放弃不再执笔也很难受啊！！权当给未来的我剖析当下精神状态的样本吧……</p><p>先引用几位大佬的话来镇镇场。</p><p>微软创始人比尔盖茨曾表示：“ChatGPT的诞生丝毫不亚于个人电脑的诞生。”英伟达创始人黄仁勋也表示：“ChatGPT只是起点，我们正处于AI的iPhone时刻。”谷歌前AI团队成员、deeplearning.ai创始人吴恩达曾在推特上发表言论，认为ChatGPT等大型语言模型是“人工智能领域最为激动人心的进展之一。”</p><p>之所以说ChatGPT打开了潘多拉魔盒是因为其影响足以推动各行各业的变革。</p><p>首先找我来说说作为一个学生，ChatGPT到底给我带来哪些影响。</p><p>ChatGPT代写课程作业(本来这个课程作业就想用它快速解决的)啥的就不多赘述了，不符合核心价值观，更多的想谈一谈ChatGPT对我知识检索习惯的改变。相信大学数学生跟我一样，在没有ChatGPT之前，我们都基于一个问题打开度娘/Google/Bing，然后输出问题的描述性语句，在通过关键词检索的词条中去检索自己需要的答案。正如我描述的一样，这一套流程下来是相当繁琐的，这意味着知识的检索成本相当之高。从这里就会给绝大部人形成一种习惯，或者是一种刻板印象：检索知识的过程是比较繁琐的，不能做到对陌生知识一遇一检。然后我们细化对知识的分类，很多陌生知识的复杂程度是不一样的，比如说你在冲浪时遇到一个陌生概念“NAS”，如果你只是想知道这个名词是个什么东西，那这个陌生知识就很轻量，因此你期待所消耗在这个陌生知识上的时间成本就应该很小，但是在ChatGPT出现之前，你还是得经历上面那个过程。但其实这么轻量的搜索，很多检索系统也能直接在第一个词条给到你想要的答案，但真正迈不出一遇一检的原因其实是你大脑对这个检索过程的定义。</p><p>基于ChatGPT的内容式问答检索，这些轻量的知识检索过程完全可以以极低的时间成本实现，真正能够做到一遇一检。这虽然不能将你的知识网络的深度延伸到一个新的层次，但是拓展你知识网络的阔度是完全没有任何问题的。</p><p>由于我大二暑假做过一个“基于2D虚拟人语音驱动”的项目，通过现有的深度学习技术已经完全可以实现模仿一个虚拟的你出来了，再配合上语音克隆的技术和ChatGPT作为对话的中转站。一个在样貌、声线、甚至是语气与微表情都和你如出一辙的虚拟人完全可以应用到生产生活的方方面面。未来注定是元宇宙的时代！</p><p>而ChatGPT对一些传统行业的打击也同样是致命的，如翻译(包括同声翻译)、在线客服等，甚至我在想，ChatGPT结合具身智能在不远的将来能否替代掉那些以时间量度作为价值换算的行业。</p><p>那么说回程序员，本来这个职业还是有蛮高的技术门槛的，一门编程语言的学习，以及其周边技术的掌握确实都是需要时间花费的。而矛盾的是，一般的人想要用到编程这项技术往往只是希望用它来减轻一些重复工作，他们是没有强烈的通过编程生产变现需求的。因此对于他们来说编程只是工具不是目的，如果可以以其他方式达到他们的目的，这个工具是可有可无的。ChatGPT的出现，完全满足了这一需求。一个脚本、一个前端界面、一个数据处理分析程序，这种轻量的编程需求，ChatGPT完全可以胜任。因此无形之中，编程好像也变得不再是程序员的专利，行业的门槛无形之中就被连根拔起。</p><p>当然对于程序员来说，这个工具的出现也是利好的。只是从我的经历看来，我完全可以将重心全放在业务功能的如何实现上，而不用太多地去考虑如何用代码实现。现在基于ChatGPT的代码补全工具也层出不穷，虽然我体验下来也还是感觉没多大作用，但我们也还是需要给这项技术足够的包容，毕竟它刚出来还不到一年。</p><p>写到这里，实在江郎才尽了！Happy Ending~</p><h2 id="参考文章-文献"><a href="#参考文章-文献" class="headerlink" title="参考文章/文献"></a>参考文章/文献</h2><ol><li><a class="link" target="_blank" rel="noopener" href="https://juejin.cn/post/7277802797473841186">让非算法同学也能了解 ChatGPT 等相关大模型 <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></li><li><a class="link" target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/370859857">教你深入理解“预训练” <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></li><li><a class="link" target="_blank" rel="noopener" href="https://arxiv.org/abs/1706.03762">Attention Is All You Need <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></li><li><a class="link" target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/338817680">Transformer模型详解（图解最完整版） <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></li></ol></div><div class="post-copyright-info w-full my-8 px-2 sm:px-6 md:px-8"><div class="article-copyright-info-container"><ul><li><strong>Title:</strong> ChatGPT 打开潘多拉魔盒，行业壁垒逐步瓦解</li><li><strong>Author:</strong> Hong Nie</li><li><strong>Created at :</strong> 2023-10-06 22:43:39</li><li><strong>Updated at :</strong> 2025-02-28 22:23:05</li><li><strong>Link:</strong> https://gme-hong.github.io/2023/10/06/ChatGPT/</li><li><strong>License: </strong>This work is licensed under <a class="license" target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0">CC BY-NC-SA 4.0</a>.</li></ul></div></div><ul class="post-tags-box text-lg mt-1.5 flex-wrap justify-center flex md:hidden"><li class="tag-item mx-0.5"><a href="/tags/AI/">#AI</a>&nbsp;</li></ul><div class="recommended-article px-2 sm:px-6 md:px-8"><div class="recommended-desktop"><div class="recommended-article-header text-xl md:text-3xl font-bold mt-10"><i aria-hidden="true"></i><span>Recommendation</span></div><div class="recommended-article-group"><a class="recommended-article-item" href="/2024/01/13/XV6-FS/" title="XV6 File System" rel="bookmark"><img src="/images/background-light.jpg" alt="XV6 File System" class="!max-w-none"> <span class="title">XV6 File System</span> </a><a class="recommended-article-item" href="/2024/11/13/Reflections/" title="一封跨越半个世纪的来信引发的思考" rel="bookmark"><img src="/images/background-light.jpg" alt="一封跨越半个世纪的来信引发的思考" class="!max-w-none"> <span class="title">一封跨越半个世纪的来信引发的思考</span> </a><a class="recommended-article-item" href="/2024/10/01/Undergraduate-Experience/" title="_._" rel="bookmark"><img src="https://ec28649.webp.li/20250228173012418.png" alt="_._" class="!max-w-none"> <span class="title">_._</span></a></div></div><div class="recommended-mobile"><div class="recommended-article-header text-xl md:text-3xl font-bold mt-10"><i aria-hidden="true"></i><span>Recommendation</span></div><div class="recommended-article-group"><a class="recommended-article-item" href="/2024/01/13/XV6-FS/" title="XV6 File System" rel="bookmark"><img src="/images/background-light.jpg" alt="XV6 File System" class="!max-w-none"> <span class="title">XV6 File System</span> </a><a class="recommended-article-item" href="/2024/11/13/Reflections/" title="一封跨越半个世纪的来信引发的思考" rel="bookmark"><img src="/images/background-light.jpg" alt="一封跨越半个世纪的来信引发的思考" class="!max-w-none"> <span class="title">一封跨越半个世纪的来信引发的思考</span></a></div></div></div><div class="article-nav my-8 flex justify-between items-center px-2 sm:px-6 md:px-8"><div class="article-prev border-border-color shadow-redefine-flat shadow-shadow-color-2 rounded-medium px-4 py-2 hover:shadow-redefine-flat-hover hover:shadow-shadow-color-2"><a class="prev" rel="prev" href="/2024/01/13/XV6-FS/"><span class="left arrow-icon flex justify-center items-center"><i class="fa-solid fa-chevron-left"></i> </span><span class="title flex justify-center items-center"><span class="post-nav-title-item">XV6 File System</span> <span class="post-nav-item">Prev posts</span></span></a></div></div><div class="comment-container px-2 sm:px-6 md:px-8 pb-8"><div class="comments-container mt-10 w-full"><div id="comment-anchor" class="w-full h-2.5"></div><div class="comment-area-title w-full my-1.5 md:my-2.5 text-xl md:text-3xl font-bold">Comments</div><div id="giscus-container"></div><script data-swup-reload-script defer>async function loadGiscus(){const t={src:"https://giscus.app/client.js","data-repo":"gme-hong/gitalk","data-repo-id":"R_kgDOLePdMQ","data-category":"General","data-category-id":"DIC_kwDOLePdMc4Cd3Gc","data-mapping":"title","data-strict":"0","data-reactions-enabled":"1","data-emit-metadata":"1","data-theme":"preferred_color_scheme","data-lang":"zh-CN","data-input-position":"bottom","data-loading":"not-lazy",crossorigin:"anonymous",async:!0},a=document.createElement("script");for(const e in t)a.setAttribute(e,t[e]);document.getElementById("giscus-container").appendChild(a)}{let t=setTimeout(()=>{loadGiscus(),clearTimeout(t)},1e3)}</script></div></div></div><div class="toc-content-container"><div class="post-toc-wrap"><div class="post-toc"><div class="toc-title">On this page</div><div class="page-title">ChatGPT 打开潘多拉魔盒，行业壁垒逐步瓦解</div><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%89%8D%E8%A8%80"><span class="nav-number">1.</span> <span class="nav-text">前言</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E7%9A%84%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86"><span class="nav-number">2.</span> <span class="nav-text">人工智能的前置知识</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0"><span class="nav-number">2.1.</span> <span class="nav-text">机器学习</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%82%E6%95%B0-%E6%9D%83%E9%87%8D%EF%BC%9A"><span class="nav-number">2.2.</span> <span class="nav-text">参数 &#x2F; 权重：</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%BF%87%E6%8B%9F%E5%90%88%E3%80%81%E6%AC%A0%E6%8B%9F%E5%90%88%E4%B8%8E%E6%A8%A1%E5%9E%8B%E9%80%80%E5%8C%96"><span class="nav-number">2.3.</span> <span class="nav-text">过拟合、欠拟合与模型退化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BA%BA%E7%B1%BB%E5%8F%8D%E9%A6%88%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%88RLHF%EF%BC%89"><span class="nav-number">2.4.</span> <span class="nav-text">人类反馈强化学习（RLHF）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-number">2.5.</span> <span class="nav-text">神经网络</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ChatGPT-%E7%9A%84%E6%A6%82%E5%BF%B5"><span class="nav-number">3.</span> <span class="nav-text">ChatGPT 的概念</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Transformer%E5%9F%BA%E6%9C%AC%E7%9F%A5%E8%AF%86"><span class="nav-number">4.</span> <span class="nav-text">Transformer基本知识</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AC%AC%E4%B8%80%E6%AD%A5%EF%BC%9Aembedding"><span class="nav-number">4.1.</span> <span class="nav-text">第一步：embedding</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AC%AC%E4%BA%8C%E6%AD%A5%EF%BC%9AAttention"><span class="nav-number">4.2.</span> <span class="nav-text">第二步：Attention</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AC%AC%E4%B8%89%E6%AD%A5%EF%BC%9A%E5%B0%86%E5%90%91%E9%87%8F%E8%BD%AC%E4%B8%BA%E6%A6%82%E7%8E%87"><span class="nav-number">4.3.</span> <span class="nav-text">第三步：将向量转为概率</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E8%AF%B4ChatGPT%E6%89%93%E5%BC%80%E4%BA%86%E6%BD%98%E5%A4%9A%E6%8B%89%E9%AD%94%E7%9B%92"><span class="nav-number">5.</span> <span class="nav-text">为什么说ChatGPT打开了潘多拉魔盒</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%82%E8%80%83%E6%96%87%E7%AB%A0-%E6%96%87%E7%8C%AE"><span class="nav-number">6.</span> <span class="nav-text">参考文章&#x2F;文献</span></a></li></ol></div></div></div></div></div></div><div class="main-content-footer"><footer class="footer mt-5 py-5 h-auto text-base text-third-text-color relative border-t-2 border-t-border-color"><div class="info-container py-3 text-center"><div class="text-center">&copy; <span>2022</span> - 2025&nbsp;&nbsp;<i class="fa-solid fa-heart fa-beat" style="--fa-animation-duration:0.5s;color:#f54545"></i>&nbsp;&nbsp;<a href="/">Hong Nie</a><p class="post-count space-x-0.5"><span>11 posts in total </span><span>43.5k words in total</span></p></div><script data-swup-reload-script src="https://cn.vercount.one/js"></script><div class="relative text-center lg:absolute lg:right-[20px] lg:top-1/2 lg:-translate-y-1/2 lg:text-right"><span id="busuanzi_container_site_uv" class="lg:!block"><span class="text-sm">VISITOR COUNT</span> <span id="busuanzi_value_site_uv"></span> </span><span id="busuanzi_container_site_pv" class="lg:!block"><span class="text-sm">TOTAL PAGE VIEWS</span> <span id="busuanzi_value_site_pv"></span></span></div><div class="relative text-center lg:absolute lg:left-[20px] lg:top-1/2 lg:-translate-y-1/2 lg:text-left"><span class="lg:block text-sm">POWERED BY <?xml version="1.0" encoding="utf-8"?><!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN" "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd"><svg class="relative top-[2px] inline-block align-baseline" version="1.1" id="圖層_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" width="1rem" height="1rem" viewBox="0 0 512 512" enable-background="new 0 0 512 512" xml:space="preserve"><path fill="#0E83CD" d="M256.4,25.8l-200,115.5L56,371.5l199.6,114.7l200-115.5l0.4-230.2L256.4,25.8z M349,354.6l-18.4,10.7l-18.6-11V275H200v79.6l-18.4,10.7l-18.6-11v-197l18.5-10.6l18.5,10.8V237h112v-79.6l18.5-10.6l18.5,10.8V354.6z"/></svg><a target="_blank" class="text-base" href="https://hexo.io">Hexo</a></span> <span class="text-sm lg:block">THEME&nbsp;<a class="text-base" target="_blank" href="https://github.com/EvanNotFound/hexo-theme-redefine">Redefine v2.7.1</a></span></div><div>Blog up for <span class="odometer" id="runtime_days"></span> days <span class="odometer" id="runtime_hours"></span> hrs <span class="odometer" id="runtime_minutes"></span> Min <span class="odometer" id="runtime_seconds"></span> Sec</div><script data-swup-reload-script>try{function odometer_init(){document.querySelectorAll(".odometer").forEach(e=>{new Odometer({el:e,format:"( ddd).dd",duration:200})})}odometer_init()}catch(e){}</script></div></footer></div></div><div class="post-tools"><div class="post-tools-container"><ul class="article-tools-list"><li class="right-bottom-tools page-aside-toggle"><i class="fa-regular fa-outdent"></i></li><li class="go-comment"><i class="fa-regular fa-comments"></i></li></ul></div></div><div class="right-side-tools-container"><div class="side-tools-container"><ul class="hidden-tools-list"><li class="right-bottom-tools tool-font-adjust-plus flex justify-center items-center"><i class="fa-regular fa-magnifying-glass-plus"></i></li><li class="right-bottom-tools tool-font-adjust-minus flex justify-center items-center"><i class="fa-regular fa-magnifying-glass-minus"></i></li><li class="right-bottom-tools tool-dark-light-toggle flex justify-center items-center"><i class="fa-regular fa-moon"></i></li><li class="right-bottom-tools tool-scroll-to-bottom flex justify-center items-center"><i class="fa-regular fa-arrow-down"></i></li></ul><ul class="visible-tools-list"><li class="right-bottom-tools toggle-tools-list flex justify-center items-center"><i class="fa-regular fa-cog fa-spin"></i></li><li class="right-bottom-tools tool-scroll-to-top flex justify-center items-center"><i class="arrow-up fas fa-arrow-up"></i> <span class="percent"></span></li></ul></div></div><div class="image-viewer-container"><img src=""></div><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-input-field-pre"><i class="fa-solid fa-keyboard"></i></span><div class="search-input-container"><input autocomplete="off" autocorrect="off" autocapitalize="off" placeholder="Search..." spellcheck="false" type="search" class="search-input"></div><span class="popup-btn-close"><i class="fa-solid fa-times"></i></span></div><div id="search-result"><div id="no-result"><i class="fa-solid fa-spinner fa-spin-pulse fa-5x fa-fw"></i></div></div></div></div></main><script src="https://registry.npmmirror.com/hexo-theme-redefine/2.7.1/files/source/js/libs/Swup.min.js"></script><script src="https://registry.npmmirror.com/hexo-theme-redefine/2.7.1/files/source/js/libs/SwupSlideTheme.min.js"></script><script src="https://registry.npmmirror.com/hexo-theme-redefine/2.7.1/files/source/js/libs/SwupScriptsPlugin.min.js"></script><script src="https://registry.npmmirror.com/hexo-theme-redefine/2.7.1/files/source/js/libs/SwupProgressPlugin.min.js"></script><script src="https://registry.npmmirror.com/hexo-theme-redefine/2.7.1/files/source/js/libs/SwupScrollPlugin.min.js"></script><script src="https://registry.npmmirror.com/hexo-theme-redefine/2.7.1/files/source/js/libs/SwupPreloadPlugin.min.js"></script><script>const swup=new Swup({plugins:[new SwupScriptsPlugin({optin:!0}),new SwupProgressPlugin,new SwupScrollPlugin({offset:80}),new SwupSlideTheme({mainElement:".main-content-body"}),new SwupPreloadPlugin],containers:["#swup"]})</script><script type="module" src="https://registry.npmmirror.com/hexo-theme-redefine/2.7.1/files/source/js/tools/imageViewer.js"></script><script type="module" src="https://registry.npmmirror.com/hexo-theme-redefine/2.7.1/files/source/js/utils.js"></script><script type="module" src="https://registry.npmmirror.com/hexo-theme-redefine/2.7.1/files/source/js/main.js"></script><script type="module" src="https://registry.npmmirror.com/hexo-theme-redefine/2.7.1/files/source/js/layouts/navbarShrink.js"></script><script type="module" src="https://registry.npmmirror.com/hexo-theme-redefine/2.7.1/files/source/js/tools/scrollTopBottom.js"></script><script type="module" src="https://registry.npmmirror.com/hexo-theme-redefine/2.7.1/files/source/js/tools/lightDarkSwitch.js"></script><script type="module" src="https://registry.npmmirror.com/hexo-theme-redefine/2.7.1/files/source/js/layouts/categoryList.js"></script><script type="module" src="https://registry.npmmirror.com/hexo-theme-redefine/2.7.1/files/source/js/tools/localSearch.js"></script><script type="module" src="https://registry.npmmirror.com/hexo-theme-redefine/2.7.1/files/source/js/tools/codeBlock.js"></script><script type="module" src="https://registry.npmmirror.com/hexo-theme-redefine/2.7.1/files/source/js/layouts/lazyload.js"></script><script src="https://registry.npmmirror.com/hexo-theme-redefine/2.7.1/files/source/js/tools/runtime.js"></script><script src="https://registry.npmmirror.com/hexo-theme-redefine/2.7.1/files/source/js/libs/odometer.min.js"></script><link rel="stylesheet" href="https://registry.npmmirror.com/hexo-theme-redefine/2.7.1/files/source/assets/odometer-theme-minimal.css"><script src="https://registry.npmmirror.com/hexo-theme-redefine/2.7.1/files/source/js/libs/Typed.min.js"></script><script type="module" src="https://registry.npmmirror.com/hexo-theme-redefine/2.7.1/files/source/js/plugins/typed.js"></script><script src="https://registry.npmmirror.com/hexo-theme-redefine/2.7.1/files/source/js/libs/mermaid.min.js"></script><script src="https://registry.npmmirror.com/hexo-theme-redefine/2.7.1/files/source/js/plugins/mermaid.js"></script><script src="https://registry.npmmirror.com/hexo-theme-redefine/2.7.1/files/source/js/libs/minimasonry.min.js"></script><script type="module" src="https://registry.npmmirror.com/hexo-theme-redefine/2.7.1/files/source/js/plugins/masonry.js"></script><script src="https://registry.npmmirror.com/hexo-theme-redefine/2.7.1/files/source/js/libs/anime.min.js"></script><div class="post-scripts" data-swup-reload-script><script type="module" src="https://registry.npmmirror.com/hexo-theme-redefine/2.7.1/files/source/js/tools/tocToggle.js"></script><script type="module" src="https://registry.npmmirror.com/hexo-theme-redefine/2.7.1/files/source/js/layouts/toc.js"></script><script type="module" src="https://registry.npmmirror.com/hexo-theme-redefine/2.7.1/files/source/js/plugins/tabs.js"></script></div></body></html>